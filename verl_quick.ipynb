{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNUCNQkezgqoFEycgA5F6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clam004/AIRL/blob/main/verl_quick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/volcengine/verl/blob/main/examples/ppo_trainer/verl_getting_started.ipynb"
      ],
      "metadata": {
        "id": "Zc8SHwqXO7ae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eZGWjvGxOnWT",
        "outputId": "5d7a83c2-bf76-45d3-c37f-b9a346fcfb80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.1.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Downloading pip-25.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.1.0\n",
            "    Uninstalling setuptools-75.1.0:\n",
            "      Successfully uninstalled setuptools-75.1.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.0 setuptools-75.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "setuptools"
                ]
              },
              "id": "a52d4c3f5f0149379470679fc776cdd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.0\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.19.0\n",
            "  Downloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (11.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu124\n",
            "    Uninstalling torchvision-0.20.1+cu124:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 torchvision-0.19.0 triton-3.0.0\n",
            "torch                              2.4.0\n",
            "torchaudio                         2.5.1+cu124\n",
            "torchsummary                       1.5.1\n",
            "torchvision                        0.19.0\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.4.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187805408 sha256=92cf49e6f66795b6934cec0cba526ed6e45d3313de3f905d45df8773f19092a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.7.4.post1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --upgrade pip setuptools wheel\n",
        "!pip3 install torch==2.4.0 torchvision==0.19.0\n",
        "!pip3 list | grep torch\n",
        "!pip3 install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/volcengine/verl $HOME/verl_repo\n",
        "!cd $HOME/verl_repo && pip3 install -e . -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdci5CzJP9Q0",
        "outputId": "20698aea-fa7f-441c-8a41-1314bb82ee96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '/root/verl_repo' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "# Restart the kernel to pickup the latest python packages\n",
        "IPython.get_ipython().kernel.do_shutdown(restart=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWI5AS1aP91o",
        "outputId": "9a47c115-8bac-43a1-c727-41334ee2cdc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "  torch.ones(1, dtype=torch.bfloat16).cuda()\n",
        "except AssertionError:\n",
        "  print(\"Please switch to an env with GPUs supporting bfloat16 (L4 RTX 5000, A5000, A100, H100, A10, etc)\")\n",
        "\n",
        "try:\n",
        "  import verl\n",
        "except Exception as e:\n",
        "  print(\"Please install verl via pip and restart the kernel\")\n",
        "  raise e\n",
        "\n",
        "import flash_attn"
      ],
      "metadata": {
        "id": "HxwNnVIlQaqt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct --local-dir $HOME/models/Qwen2.5-0.5B-Instruct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBBZc4lwQg-6",
        "outputId": "a0019cbb-063c-481d-9c55-34c5aa9634a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching 10 files:   0% 0/10 [00:00<?, ?it/s]Downloading '.gitattributes' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
            "Downloading 'model.safetensors' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/xGOKKLRSlIhH692hSVvI1-gpoa8=.fdf756fa7fcbe7404d5c60e26bff1a0c8b8aa1f72ced49e7dd0210fe288fb7fe.incomplete'\n",
            "Downloading 'README.md' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.4b8373851d093eb9f3017443f27781c6971eff24.incomplete'\n",
            "Downloading 'LICENSE' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/DhCjcNQuMpl4FL346qr3tvNUCgY=.6634c8cc3133b3848ec74b9f275acaaa1ea618ab.incomplete'\n",
            "\n",
            "model.safetensors:   0% 0.00/988M [00:00<?, ?B/s]\u001b[ADownloading 'config.json' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.0dbb161213629a23f0fc00ef286e6b1e366d180f.incomplete'\n",
            "Downloading 'generation_config.json' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.dfc11073787daf1b0f9c0f1499487ab5f4c93738.incomplete'\n",
            "Downloading 'tokenizer.json' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.443909a61d429dff23010e5bddd28ff530edda00.incomplete'\n",
            "Downloading 'merges.txt' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/PtHk0z_I45atnj23IIRhTExwT3w=.20024bfe7c83998e9aeaf98a0cd6a2ce6306c2f0.incomplete'\n",
            "\n",
            "\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 8.26MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/.gitattributes\n",
            "Fetching 10 files:  10% 1/10 [00:00<00:04,  2.11it/s]\n",
            "model.safetensors:   1% 10.5M/988M [00:00<00:23, 41.6MB/s]\u001b[A\n",
            "\n",
            "LICENSE: 100% 11.3k/11.3k [00:00<00:00, 41.9MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/LICENSE\n",
            "\n",
            "\n",
            "config.json: 100% 659/659 [00:00<00:00, 5.56MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/config.json\n",
            "\n",
            "\n",
            "merges.txt:   0% 0.00/1.67M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.json:   0% 0.00/7.03M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "README.md: 100% 4.92k/4.92k [00:00<00:00, 25.3MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/README.md\n",
            "Fetching 10 files:  40% 4/10 [00:00<00:00,  8.56it/s]\n",
            "\n",
            "\n",
            "\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 1.70MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/generation_config.json\n",
            "Downloading 'tokenizer_config.json' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.07bfe0640cb5a0037f9322287fbfc682806cf672.incomplete'\n",
            "Downloading 'vocab.json' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/j3m-Hy6QvBddw8RXA1uSWl1AJ0c=.4783fe10ac3adce15ac8f358ef5462739852c569.incomplete'\n",
            "\n",
            "model.safetensors:   2% 21.0M/988M [00:00<00:22, 42.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vocab.json:   0% 0.00/2.78M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   3% 31.5M/988M [00:00<00:22, 42.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 100% 7.30k/7.30k [00:00<00:00, 32.0MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/tokenizer_config.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 12.5MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/vocab.json\n",
            "\n",
            "model.safetensors:   4% 41.9M/988M [00:00<00:22, 42.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 7.89MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/tokenizer.json\n",
            "\n",
            "model.safetensors:   5% 52.4M/988M [00:01<00:21, 42.8MB/s]\u001b[A\n",
            "model.safetensors:   6% 62.9M/988M [00:01<00:21, 42.8MB/s]\u001b[A\n",
            "\n",
            "merges.txt: 100% 1.67M/1.67M [00:01<00:00, 1.29MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/merges.txt\n",
            "Fetching 10 files:  60% 6/10 [00:01<00:01,  2.92it/s]\n",
            "model.safetensors:   7% 73.4M/988M [00:01<00:21, 42.7MB/s]\u001b[A\n",
            "model.safetensors:   8% 83.9M/988M [00:01<00:21, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  10% 94.4M/988M [00:02<00:20, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  11% 105M/988M [00:02<00:20, 42.8MB/s] \u001b[A\n",
            "model.safetensors:  12% 115M/988M [00:02<00:20, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  13% 126M/988M [00:02<00:20, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  14% 136M/988M [00:03<00:20, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  15% 147M/988M [00:03<00:19, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  16% 157M/988M [00:03<00:19, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  17% 168M/988M [00:03<00:19, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  18% 178M/988M [00:04<00:18, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  19% 189M/988M [00:04<00:18, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  20% 199M/988M [00:04<00:18, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  21% 210M/988M [00:04<00:18, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  22% 220M/988M [00:05<00:17, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  23% 231M/988M [00:05<00:17, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  24% 241M/988M [00:05<00:17, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  25% 252M/988M [00:05<00:17, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  27% 262M/988M [00:06<00:16, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  28% 273M/988M [00:06<00:16, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  29% 283M/988M [00:06<00:16, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  30% 294M/988M [00:06<00:16, 42.3MB/s]\u001b[A\n",
            "model.safetensors:  31% 304M/988M [00:07<00:16, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  32% 315M/988M [00:07<00:15, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  33% 325M/988M [00:07<00:15, 42.3MB/s]\u001b[A\n",
            "model.safetensors:  34% 336M/988M [00:07<00:15, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  35% 346M/988M [00:08<00:14, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  36% 357M/988M [00:08<00:14, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  37% 367M/988M [00:08<00:14, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  38% 377M/988M [00:08<00:14, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  39% 388M/988M [00:09<00:14, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  40% 398M/988M [00:09<00:13, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  41% 409M/988M [00:09<00:13, 42.9MB/s]\u001b[A\n",
            "model.safetensors:  42% 419M/988M [00:09<00:13, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  44% 430M/988M [00:10<00:13, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  45% 440M/988M [00:10<00:12, 42.9MB/s]\u001b[A\n",
            "model.safetensors:  46% 451M/988M [00:10<00:12, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  47% 461M/988M [00:10<00:12, 41.7MB/s]\u001b[A\n",
            "model.safetensors:  48% 472M/988M [00:11<00:12, 42.2MB/s]\u001b[A\n",
            "model.safetensors:  49% 482M/988M [00:11<00:13, 37.4MB/s]\u001b[A\n",
            "model.safetensors:  50% 493M/988M [00:11<00:12, 39.0MB/s]\u001b[A\n",
            "model.safetensors:  51% 503M/988M [00:11<00:12, 39.8MB/s]\u001b[A\n",
            "model.safetensors:  52% 514M/988M [00:12<00:11, 40.5MB/s]\u001b[A\n",
            "model.safetensors:  53% 524M/988M [00:12<00:11, 40.9MB/s]\u001b[A\n",
            "model.safetensors:  54% 535M/988M [00:12<00:10, 41.4MB/s]\u001b[A\n",
            "model.safetensors:  55% 545M/988M [00:12<00:10, 41.7MB/s]\u001b[A\n",
            "model.safetensors:  56% 556M/988M [00:13<00:10, 42.0MB/s]\u001b[A\n",
            "model.safetensors:  57% 566M/988M [00:13<00:10, 42.1MB/s]\u001b[A\n",
            "model.safetensors:  58% 577M/988M [00:13<00:09, 42.3MB/s]\u001b[A\n",
            "model.safetensors:  59% 587M/988M [00:13<00:09, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  60% 598M/988M [00:14<00:09, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  62% 608M/988M [00:14<00:08, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  63% 619M/988M [00:14<00:08, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  64% 629M/988M [00:14<00:08, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  65% 640M/988M [00:15<00:08, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  66% 650M/988M [00:15<00:07, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  67% 661M/988M [00:15<00:07, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  68% 671M/988M [00:15<00:07, 42.9MB/s]\u001b[A\n",
            "model.safetensors:  69% 682M/988M [00:16<00:07, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  70% 692M/988M [00:16<00:06, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  71% 703M/988M [00:16<00:06, 42.4MB/s]\u001b[A\n",
            "model.safetensors:  72% 713M/988M [00:16<00:06, 42.4MB/s]\u001b[A\n",
            "model.safetensors:  73% 724M/988M [00:17<00:06, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  74% 734M/988M [00:17<00:05, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  75% 744M/988M [00:17<00:05, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  76% 755M/988M [00:17<00:05, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  77% 765M/988M [00:18<00:05, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  79% 776M/988M [00:18<00:04, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  80% 786M/988M [00:18<00:04, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  81% 797M/988M [00:18<00:04, 41.8MB/s]\u001b[A\n",
            "model.safetensors:  82% 807M/988M [00:19<00:04, 42.0MB/s]\u001b[A\n",
            "model.safetensors:  83% 818M/988M [00:19<00:04, 42.3MB/s]\u001b[A\n",
            "Fetching 10 files:  60% 6/10 [00:19<00:01,  2.92it/s]\n",
            "model.safetensors:  85% 839M/988M [00:19<00:03, 41.6MB/s]\u001b[A\n",
            "model.safetensors:  86% 849M/988M [00:20<00:03, 42.1MB/s]\u001b[A\n",
            "model.safetensors:  87% 860M/988M [00:20<00:03, 42.3MB/s]\u001b[A\n",
            "model.safetensors:  88% 870M/988M [00:20<00:02, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  89% 881M/988M [00:20<00:02, 41.3MB/s]\u001b[A\n",
            "model.safetensors:  90% 891M/988M [00:21<00:02, 41.8MB/s]\u001b[A\n",
            "model.safetensors:  91% 902M/988M [00:21<00:02, 42.2MB/s]\u001b[A\n",
            "model.safetensors:  92% 912M/988M [00:21<00:01, 42.3MB/s]\u001b[A\n",
            "model.safetensors:  93% 923M/988M [00:21<00:01, 41.4MB/s]\u001b[A\n",
            "model.safetensors:  94% 933M/988M [00:22<00:01, 42.0MB/s]\u001b[A\n",
            "model.safetensors:  96% 944M/988M [00:22<00:01, 42.1MB/s]\u001b[A\n",
            "model.safetensors:  97% 954M/988M [00:22<00:00, 42.1MB/s]\u001b[A\n",
            "model.safetensors:  98% 965M/988M [00:22<00:00, 41.5MB/s]\u001b[A\n",
            "model.safetensors:  99% 975M/988M [00:23<00:00, 42.1MB/s]\u001b[A\n",
            "model.safetensors: 100% 988M/988M [00:23<00:00, 42.3MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/model.safetensors\n",
            "Fetching 10 files: 100% 10/10 [00:23<00:00,  2.37s/it]\n",
            "/root/models/Qwen2.5-0.5B-Instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p $HOME/data/gsm8k\n",
        "!python3 $HOME/verl_repo/examples/data_preprocess/gsm8k.py --local_dir $HOME/data/gsm8k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehjQ07mAQohD",
        "outputId": "00ddcc3c-0b71-4be4-c4f3-cc87e6b70652"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md: 100% 7.94k/7.94k [00:00<00:00, 31.3MB/s]\n",
            "train-00000-of-00001.parquet: 100% 2.31M/2.31M [00:00<00:00, 157MB/s]\n",
            "test-00000-of-00001.parquet: 100% 419k/419k [00:00<00:00, 372MB/s]\n",
            "Generating train split: 100% 7473/7473 [00:00<00:00, 139599.04 examples/s]\n",
            "Generating test split: 100% 1319/1319 [00:00<00:00, 314173.83 examples/s]\n",
            "Map: 100% 7473/7473 [00:00<00:00, 19969.45 examples/s]\n",
            "Map: 100% 1319/1319 [00:00<00:00, 20110.75 examples/s]\n",
            "Creating parquet from Arrow format: 100% 8/8 [00:00<00:00, 228.24ba/s]\n",
            "Creating parquet from Arrow format: 100% 2/2 [00:00<00:00, 288.14ba/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "from verl.utils.reward_score.gsm8k import compute_score as gsm8k_reward\n",
        "print(inspect.getsource(gsm8k_reward))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N29CiYH_Qu8n",
        "outputId": "02ddd3c5-8849-4a00-c948-48e11f0d4054"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def compute_score(solution_str, ground_truth, method='strict', format_score=0., score=1.):\n",
            "    \"\"\"The scoring function for GSM8k.\n",
            "\n",
            "    Reference: Trung, Luong, et al. \"Reft: Reasoning with reinforced fine-tuning.\" Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024.\n",
            "\n",
            "    Args:\n",
            "        solution_str: the solution text\n",
            "        ground_truth: the ground truth\n",
            "        method: the method to extract the solution, choices are 'strict' and 'flexible'\n",
            "        format_score: the score for the format\n",
            "        score: the score for the correct answer\n",
            "    \"\"\"\n",
            "    answer = extract_solution(solution_str=solution_str, method=method)\n",
            "    if answer is None:\n",
            "        return 0\n",
            "    else:\n",
            "        if answer == ground_truth:\n",
            "            return score\n",
            "        else:\n",
            "            return format_score\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n",
        " data.train_files=$HOME/data/gsm8k/train.parquet \\\n",
        " data.val_files=$HOME/data/gsm8k/test.parquet \\\n",
        " data.train_batch_size=256 \\\n",
        " data.val_batch_size=1312 \\\n",
        " data.max_prompt_length=512 \\\n",
        " data.max_response_length=256 \\\n",
        " actor_rollout_ref.model.path=$HOME/models/Qwen2.5-0.5B-Instruct \\\n",
        " actor_rollout_ref.actor.optim.lr=1e-6 \\\n",
        " actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n",
        " actor_rollout_ref.actor.ppo_micro_batch_size=1 \\\n",
        " actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=1 \\\n",
        " actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n",
        " actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n",
        " actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n",
        " critic.optim.lr=1e-5 \\\n",
        " critic.model.path=$HOME/models/Qwen2.5-0.5B-Instruct \\\n",
        " critic.ppo_micro_batch_size=1 \\\n",
        " algorithm.kl_ctrl.kl_coef=0.001 \\\n",
        " +trainer.val_before_train=False \\\n",
        " trainer.default_hdfs_dir=null \\\n",
        " trainer.n_gpus_per_node=1 \\\n",
        " trainer.nnodes=1 \\\n",
        " trainer.save_freq=10 \\\n",
        " trainer.test_freq=10 \\\n",
        " trainer.total_epochs=15 \\\n",
        " trainer.logger=\\[console\\]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFkSnec3Q0h_",
        "outputId": "ada275fa-9554-4887-f750-7bc2e8423aa6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
            "2025-02-08 15:52:46.477898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739029966.725015    2752 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739029966.792455    2752 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-08 15:52:47.319585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-02-08 15:52:54,502\tINFO worker.py:1841 -- Started a local Ray instance.\n",
            "\u001b[36m(pid=3049)\u001b[0m 2025-02-08 15:53:02.972096: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=3049)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=3049)\u001b[0m E0000 00:00:1739029983.005693    3049 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=3049)\u001b[0m E0000 00:00:1739029983.017994    3049 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'entropy_coeff': 0.001,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'fsdp_config': {'fsdp_size': -1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                                  'grad_offload': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                                  'optimizer_offload': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                                  'param_offload': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                                  'wrap_policy': {'min_num_params': 0}},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'grad_clip': 1.0,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'kl_loss_coef': 0.001,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'kl_loss_type': 'low_var_kl',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'optim': {'lr': 1e-06,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                            'lr_warmup_steps_ratio': 0.0,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                            'min_lr_ratio': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                            'total_training_steps': -1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                            'warmup_style': 'constant'},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'ppo_epochs': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'ppo_max_token_len_per_gpu': 16384,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'ppo_micro_batch_size': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'ppo_micro_batch_size_per_gpu': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'ppo_mini_batch_size': 64,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'shuffle': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'strategy': 'fsdp',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'ulysses_sequence_parallel_size': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'use_dynamic_bsz': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'use_kl_loss': False},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                        'hybrid_engine': True,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                        'model': {'enable_gradient_checkpointing': True,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'external_lib': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'override_config': {},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'path': '/root/models/Qwen2.5-0.5B-Instruct',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                  'use_remove_padding': False},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                        'ref': {'fsdp_config': {'param_offload': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                                'wrap_policy': {'min_num_params': 0}},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                'log_prob_max_token_len_per_gpu': 16384,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                'log_prob_micro_batch_size': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                'log_prob_micro_batch_size_per_gpu': 4,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                'log_prob_use_dynamic_bsz': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                'ulysses_sequence_parallel_size': 1},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                        'rollout': {'disable_log_stats': True,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'do_sample': True,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'dtype': 'bfloat16',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'enable_chunked_prefill': True,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'enforce_eager': True,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'free_cache_engine': True,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'gpu_memory_utilization': 0.4,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'ignore_eos': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'load_format': 'dummy_dtensor',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'log_prob_max_token_len_per_gpu': 16384,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'log_prob_micro_batch_size': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'log_prob_micro_batch_size_per_gpu': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'log_prob_use_dynamic_bsz': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'max_num_batched_tokens': 8192,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'max_num_seqs': 1024,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'n': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'name': 'vllm',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'prompt_length': 512,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'response_length': 256,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'temperature': 1.0,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'tensor_model_parallel_size': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'top_k': -1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                    'top_p': 1}},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m  'algorithm': {'adv_estimator': 'gae',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                'gamma': 1.0,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                'kl_ctrl': {'kl_coef': 0.001, 'type': 'fixed'},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                'kl_penalty': 'kl',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                'lam': 1.0},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m  'critic': {'cliprange_value': 0.5,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'forward_max_token_len_per_gpu': 32768,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'forward_micro_batch_size': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'forward_micro_batch_size_per_gpu': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'grad_clip': 1.0,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'model': {'enable_gradient_checkpointing': True,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                       'external_lib': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                       'fsdp_config': {'fsdp_size': -1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                       'grad_offload': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                       'optimizer_offload': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                       'param_offload': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                       'wrap_policy': {'min_num_params': 0}},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                       'override_config': {},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                       'path': '/root/models/Qwen2.5-0.5B-Instruct',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                       'tokenizer_path': '/root/models/Qwen2.5-0.5B-Instruct',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                       'use_remove_padding': False},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'optim': {'lr': 1e-05,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                       'lr_warmup_steps_ratio': 0.0,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                       'min_lr_ratio': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                       'total_training_steps': -1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                       'warmup_style': 'constant'},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'ppo_epochs': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'ppo_max_token_len_per_gpu': 32768,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'ppo_micro_batch_size': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'ppo_micro_batch_size_per_gpu': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'ppo_mini_batch_size': 64,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'shuffle': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'strategy': 'fsdp',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'ulysses_sequence_parallel_size': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m             'use_dynamic_bsz': False},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m  'data': {'max_prompt_length': 512,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m           'max_response_length': 256,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m           'prompt_key': 'prompt',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m           'return_raw_chat': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m           'return_raw_input_ids': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m           'shuffle': True,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m           'tokenizer': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m           'train_batch_size': 256,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m           'train_files': '/root/data/gsm8k/train.parquet',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m           'val_batch_size': 1312,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m           'val_files': '/root/data/gsm8k/test.parquet'},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m  'reward_model': {'enable': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                   'forward_max_token_len_per_gpu': 32768,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                   'max_length': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                   'micro_batch_size': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                   'micro_batch_size_per_gpu': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                   'model': {'external_lib': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                             'fsdp_config': {'fsdp_size': -1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                             'min_num_params': 0,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                                             'param_offload': False},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                             'input_tokenizer': '/root/models/Qwen2.5-0.5B-Instruct',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                             'use_remove_padding': False},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                   'strategy': 'fsdp',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                   'ulysses_sequence_parallel_size': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m                   'use_dynamic_bsz': False},\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m  'trainer': {'critic_warmup': 0,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'default_hdfs_dir': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'default_local_dir': 'checkpoints/verl_examples/gsm8k',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'experiment_name': 'gsm8k',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'logger': ['console'],\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'n_gpus_per_node': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'nnodes': 1,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'project_name': 'verl_examples',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'resume_from_path': False,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'resume_mode': 'auto',\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'save_freq': 10,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'test_freq': 10,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'total_epochs': 15,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'total_training_steps': None,\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m              'val_before_train': False}}\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m /usr/local/lib/python3.11/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m No module named 'vllm._version'\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m   from vllm.version import __version__ as VLLM_VERSION\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m [validate_config] All configuration checks passed successfully!\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m original dataset len: 7473\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m filter dataset len: 7473\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m original dataset len: 1319\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m filter dataset len: 1319\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m Size of train dataloader: 29\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m Size of val dataloader: 1\n",
            "\u001b[36m(main_task pid=3049)\u001b[0m Total training steps: 435\n",
            "\u001b[36m(pid=3207)\u001b[0m 2025-02-08 15:53:20.145593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=3207)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=3207)\u001b[0m E0000 00:00:1739030000.169207    3207 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=3207)\u001b[0m E0000 00:00:1739030000.178274    3207 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=3207)\u001b[0m /usr/local/lib/python3.11/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
            "\u001b[36m(pid=3207)\u001b[0m No module named 'vllm._version'\n",
            "\u001b[36m(pid=3207)\u001b[0m   from vllm.version import __version__ as VLLM_VERSION\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Critic overriding config {'bos_token_id': None, 'eos_token_id': 151645, 'pad_token_id': 151643}\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForTokenClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Qwen2ForTokenClassification contains 494.03M parameters\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Before critic FSDP, memory allocated (GB): 0.0, memory reserved (GB): 0.0\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at /root/models/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.bias', 'score.weight']\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m NCCL version 2.20.5+cuda12.4\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m After critic FSDP, memory allocated (GB): 1.8410954475402832, memory reserved (GB): 2.95703125\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Total steps: 435, num_warmup_steps: 0\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Critic use_remove_padding=False\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Model config after override: Qwen2Config {\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"_name_or_path\": \"/root/models/Qwen2.5-0.5B-Instruct\",\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"architectures\": [\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m     \"Qwen2ForCausalLM\"\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   ],\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"attention_dropout\": 0.0,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"eos_token_id\": 151645,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"hidden_act\": \"silu\",\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"hidden_size\": 896,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"initializer_range\": 0.02,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"intermediate_size\": 4864,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"max_position_embeddings\": 32768,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"max_window_layers\": 21,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"model_type\": \"qwen2\",\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"num_attention_heads\": 14,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"num_hidden_layers\": 24,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"num_key_value_heads\": 2,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"pad_token_id\": 151643,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"rope_scaling\": null,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"rope_theta\": 1000000.0,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"sliding_window\": null,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"tie_word_embeddings\": true,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"transformers_version\": \"4.47.1\",\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"use_cache\": true,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"use_sliding_window\": false,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"vocab_size\": 151936\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m }\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m \n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Qwen2ForCausalLM contains 494.03M parameters\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m wrap_policy: functools.partial(<function _or_policy at 0x7c8bd67f4f40>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7c8bd67f4e00>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Actor use_remove_padding=False\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Model config after override: Qwen2Config {\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"_name_or_path\": \"/root/models/Qwen2.5-0.5B-Instruct\",\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"architectures\": [\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m     \"Qwen2ForCausalLM\"\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   ],\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"attention_dropout\": 0.0,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"eos_token_id\": 151645,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"hidden_act\": \"silu\",\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"hidden_size\": 896,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"initializer_range\": 0.02,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"intermediate_size\": 4864,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"max_position_embeddings\": 32768,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"max_window_layers\": 21,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"model_type\": \"qwen2\",\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"num_attention_heads\": 14,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"num_hidden_layers\": 24,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"num_key_value_heads\": 2,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"pad_token_id\": 151643,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"rope_scaling\": null,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"rope_theta\": 1000000.0,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"sliding_window\": null,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"tie_word_embeddings\": true,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"transformers_version\": \"4.47.1\",\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"use_cache\": true,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"use_sliding_window\": false,\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   \"vocab_size\": 151936\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m }\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m \n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Qwen2ForCausalLM contains 494.03M parameters\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m wrap_policy: functools.partial(<function _or_policy at 0x7c8bd67f4f40>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7c8bd67f4e00>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Total steps: 435, num_warmup_steps: 0\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Actor use_remove_padding=False\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m Before building vllm rollout, memory allocated (GB): 4.602750778198242, memory reserved (GB): 5.78125\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m INFO 02-08 15:53:49 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m WARNING 02-08 15:53:49 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m local rank 0\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m INFO 02-08 15:53:49 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m INFO 02-08 15:53:49 selector.py:115] Using XFormers backend.\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m /usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m /usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "\u001b[36m(WorkerDict pid=3207)\u001b[0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
            "Error executing job with overrides: ['data.train_files=/root/data/gsm8k/train.parquet', 'data.val_files=/root/data/gsm8k/test.parquet', 'data.train_batch_size=256', 'data.val_batch_size=1312', 'data.max_prompt_length=512', 'data.max_response_length=256', 'actor_rollout_ref.model.path=/root/models/Qwen2.5-0.5B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.actor.ppo_mini_batch_size=64', 'actor_rollout_ref.actor.ppo_micro_batch_size=1', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=1', 'actor_rollout_ref.rollout.tensor_model_parallel_size=1', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.4', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4', 'critic.optim.lr=1e-5', 'critic.model.path=/root/models/Qwen2.5-0.5B-Instruct', 'critic.ppo_micro_batch_size=1', 'algorithm.kl_ctrl.kl_coef=0.001', '+trainer.val_before_train=False', 'trainer.default_hdfs_dir=null', 'trainer.n_gpus_per_node=1', 'trainer.nnodes=1', 'trainer.save_freq=10', 'trainer.test_freq=10', 'trainer.total_epochs=15', 'trainer.logger=[console]']\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/verl_repo/verl/trainer/main_ppo.py\", line 98, in main\n",
            "    run_ppo(config)\n",
            "  File \"/root/verl_repo/verl/trainer/main_ppo.py\", line 106, in run_ppo\n",
            "    ray.get(main_task.remote(config, compute_score))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2772, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 919, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::main_task()\u001b[39m (pid=3049, ip=172.28.0.12)\n",
            "  File \"/root/verl_repo/verl/trainer/main_ppo.py\", line 191, in main_task\n",
            "    trainer.init_workers()\n",
            "  File \"/root/verl_repo/verl/trainer/ppo/ray_trainer.py\", line 605, in init_workers\n",
            "    self.actor_rollout_wg.init_model()\n",
            "  File \"/root/verl_repo/verl/single_controller/ray/base.py\", line 42, in func\n",
            "    output = ray.get(output)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::WorkerDict.actor_rollout_init_model()\u001b[39m (pid=3207, ip=172.28.0.12, actor_id=598e89dd67e4c60b3314c19201000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7c8b251e8c50>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/verl_repo/verl/single_controller/ray/base.py\", line 399, in func\n",
            "    return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/verl_repo/verl/single_controller/base/decorator.py\", line 404, in inner\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/verl_repo/verl/workers/fsdp_workers.py\", line 373, in init_model\n",
            "    self.rollout, self.rollout_sharding_manager = self._build_rollout()\n",
            "                                                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/verl_repo/verl/workers/fsdp_workers.py\", line 307, in _build_rollout\n",
            "    rollout = vLLMRollout(actor_module=self.actor_module_fsdp,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/verl_repo/verl/workers/rollout/vllm_rollout/vllm_rollout.py\", line 92, in __init__\n",
            "    self.inference_engine = LLM(\n",
            "                            ^^^^\n",
            "  File \"/root/verl_repo/verl/third_party/vllm/vllm_v_0_6_3/llm.py\", line 142, in __init__\n",
            "    self.llm_engine = LLMEngine.from_engine_args(model, tokenizer, engine_args)  # TODO: check usagecontext\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/verl_repo/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py\", line 393, in from_engine_args\n",
            "    engine = cls(\n",
            "             ^^^^\n",
            "  File \"/root/verl_repo/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py\", line 212, in __init__\n",
            "    self.model_executor = executor_class(\n",
            "                          ^^^^^^^^^^^^^^^\n",
            "  File \"/root/verl_repo/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py\", line 71, in __init__\n",
            "    self._init_executor(model, distributed_init_method)\n",
            "  File \"/root/verl_repo/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py\", line 78, in _init_executor\n",
            "    self._init_workers_sp(model, distributed_init_method)\n",
            "  File \"/root/verl_repo/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py\", line 111, in _init_workers_sp\n",
            "    self.worker.init_device()\n",
            "  File \"/root/verl_repo/verl/third_party/vllm/vllm_v_0_6_3/worker.py\", line 163, in init_device\n",
            "    _check_if_gpu_supports_dtype(self.model_config.dtype)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\", line 473, in _check_if_gpu_supports_dtype\n",
            "    raise ValueError(\n",
            "ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla T4 GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ]
}